---
title: "PCSF WGCNA analysis"
output: html_document
date: "2023-11-07"
---

```{r load packages, include=FALSE}
library(PCSF)
library(igraph)
library(tidyverse)
library(reshape2)
library(biomaRt)
library(RobustRankAggreg)
library(gridExtra)
library(enrichR)
library(kableExtra)
library(WGCNA)
library(edgeR)
library(ggrepel)

ensembl <- useMart("ensembl", dataset = "hsapiens_gene_ensembl")
```

```{r setup, include=FALSE}
# combine stages
ben <- read.csv("rna_seq_data/ben_master_df.csv")
stage_I <- read.csv("rna_seq_data/stage_I_master_df.csv")
stage_II <- read.csv("rna_seq_data/stage_II_master_df.csv")
stage_III <- read.csv("rna_seq_data/stage_III_master_df.csv")
stage_IV <- read.csv("rna_seq_data/stage_IV_master_df.csv")

kylie_data <- merge(ben, stage_I,)
kylie_data <- merge(kylie_data, stage_II, by = "X")
kylie_data <- merge(kylie_data, stage_III, by = "X")
kylie_data <- merge(kylie_data, stage_IV, by = "X")

colnames(kylie_data)[1] <- "gene_id"
rownames(kylie_data) <- kylie_data$gene_id
kylie_data <- kylie_data[, -1] # remove gene ids for calcs


# create stage ID for each sample
stage_info <- data.frame(
  Sample = colnames(kylie_data),
  Stage = character(length(colnames(kylie_data)))
)

stages <- c("ben", "stage_I", "stage_II", "stage_III", "stage_IV")

for (stage in stages) {
  stage_data <- get(stage)
  stage_info$Stage[colnames(kylie_data) %in% colnames(stage_data)] <- stage
}


## removing 0 Variance and low activity genes

# Identify outliers. Searches for 0 variance genes and missing entries
gsg <- goodSamplesGenes(t(kylie_data))
summary(gsg)
gsg$allOK



# top x% of samples
num_genes <- nrow(kylie_data)
top_threshold <- ceiling(num_genes * 0.50)

# min required samples
num_samples <- ncol(kylie_data)
min_required_samples <- ceiling(num_samples * 0.10)



# Extract the gene names and data columns
gene_names <- rownames(kylie_data)
data_columns <- kylie_data

# Initialise an empty data frame
results_df <- data.frame(gene_id = gene_names)

# Iterate through each data column and perform the test
for (col_id in seq_along(data_columns)) {
  # Get the column name and the data
  col_name <- colnames(data_columns)[col_id]
  col_data <- data_columns[, col_id]
  
  # Sort the data and mark genes within the top_threshold rows as TRUE
  sorted_indices <- order(col_data, decreasing = TRUE)
  top_indices <- sorted_indices[1:top_threshold]
  
  # Create a logical vector for gene presence in top_threshold
  gene_presence <- rep(FALSE, nrow(kylie_data))
  gene_presence[top_indices] <- TRUE
  
  # Add to the results data frame
  results_df <- cbind(results_df, gene_presence)
}

# Rename columns 
colnames(results_df) <- c("gene_id", colnames(data_columns))


# Calculate the number of TRUE values for each gene across columns
gene_counts <- rowSums(results_df[, -1]) # Exclude the first column ("gene_id")
failed_genes <- gene_counts < min_required_samples
print(summary(failed_genes))

wgcna_data <- subset(kylie_data, !(rownames(kylie_data) %in% gene_names[failed_genes]))
```

<br>

## Description

In this pipeline MOC RNA-Seq data is analysed using WGCNA to obtain two sepetarate co-expression networks for benign and disease groups. Several filtering steps were undertaken in order to reduce noise:

-   Initial gene set consists of 14,889 genes
-   The goodSamplesGenes function was run to check for "bad" samples and to check for 0 variance genes
-   Low activity genes were then removed by creating a threshold whereby each gene must appear in the top 50% of genes within a sample in at least 10% of the samples
-   Counts were converted to counts per million (CPM) to account for large differences library size

<details>

<summary>Library size comparison of CPM vs Raw counts</summary>

```{r library size plot,  echo=FALSE, out.width='50%', fig.show='hold'}
raw <- barplot(colSums(wgcna_data), 
        xaxt = "n",
        xlab = "Samples",
        ylab = "Raw Counts")

wgcna_data <- cpm(wgcna_data, log = T)
cpm <- barplot(colSums(wgcna_data), 
        xaxt = "n",
        xlab = "Samples",
        ylab = "CPM")

```

</details>

<br>

-   Next a PCA was performed to observe the data dimensions. Initially, 3 outliers were removed but then decided to be kept in as there are not enough samples to make a definitive decision. Additionally, cancer samples and patients are inherently different and must be accounted for.

<details>

<summary>PCA plot</summary>

```{r PCA analysis, echo=FALSE}
pca <- prcomp(t(wgcna_data))
pca_data <- pca$x
pca_var <- pca$sdev^2

pca_var_perc <- round(pca_var/sum(pca_var)*100, digits = 2)

pca_data <- as.data.frame(pca_data)


# Merge sample-stage mapping with PCA data
pca_data <- merge(pca_data, stage_info, by.x = "row.names", by.y = "Sample")

# Create a custom color palette for stages
stage_colors <- c("ben" = "blue", "stage_I" = "green", "stage_II" = "red", "stage_III" = "purple", "stage_IV" = "orange")

# Create the PCA plot with color mapping
ggplot(pca_data, aes(PC1, PC2, color = Stage)) +
  geom_point() +
  geom_text_repel(aes(label = row.names(pca_data)), size = 3) +  # Adjust the label size here
  scale_color_manual(values = stage_colors) + # Use the custom color palette
  theme_bw() +
  labs(x = paste0('PC1: ', pca_var_perc[1], ' %'),
       y = paste0('PC2: ', pca_var_perc[2], ' %'))
```

</details>

<br>

-   prior to WGCNA analysis, the gene set consists of 9,934 genes.

<br>
*   The adjacency function was then utlised from the WGCNA which calculates (correlation or distance) network adjacency from given expression data

*   A differential network was then generated using the two groups (benign & disease) by performing the diff_i method.

<details>

<summary>Diff_i method</summary>

```{r diff_i, eval=FALSE}
sum_matrix <- disease_adj + benign_adj
normalised_scores <- apply(sum_matrix, 2, max)
normalised_scores <- sum_matrix / normalised_scores
median <- rowMedians(normalised_scores)
differential_weights <- normalised_scores - median
```

</details>

<br>

*   This differential network (large) was then fed into PCSF as the reference interactome and weighted with consequence rank scores from SNP data from Kylie's files. Some genes had several mutations of varying consequence rank, therefore, a mean rank was taken per gene which resulted in 11,731 unique genes. Of these genes, 11,503 had a corresponding ensembl gene ID.

<details>

<summary>Kylie consequence data</summary>

```{r kylies variant data, echo=FALSE}
mocVariantData <- read.csv("PCSF/data/mocVariantData.csv", header = T, na.strings = ".")
mean_rank_by_gene <- aggregate(Consequence_Rank ~ SYMBOL, data = mocVariantData, FUN = mean)

gene_ensembl <- getBM(attributes = c("external_gene_name", "ensembl_gene_id"), 
                      filters = "external_gene_name", 
                      values = mean_rank_by_gene$SYMBOL, 
                      mart = ensembl)

mean_consequence <- merge(gene_ensembl, mean_rank_by_gene, by.x = "external_gene_name", by.y = "SYMBOL")
mean_consequence <- mean_consequence[, -1]

head(mocVariantData, n=20)
```

</details>

*   The PCSF run took approximately 30hrs with 10 iterations, adding random noise each time. PCSF was run with the following parameters/function:

```{r PCSF run function, eval=FALSE}
subnet <- PCSF_rand(interactome, terminals, n = 10, r = 0.1, w = 2, b = 1, mu = 0.0005)
```

<br>

The resulting subnetwork consisted of 5,101 genes which are in gene ensembles and are required to be converted to Uniprot IDs in order to be cross referenced with proteomic data. 36 genes did not have a corresponding uniprot ID and many genes had mulitple associated uniprot IDs, resulting in a list of 26,780 uniprot IDs.

<br>

The data was then merged with the Fpocket and citation scoring data to create the same data table as done in previous pipelines.

<br>

```{r PCSF data table and name conversion, echo=FALSE}
load("PCSF/data/PCSF_subnet(n=10).RData")

# extract cluster data
clust <- clusters(subnet)
df <- data.frame(gene_id = names(clust$membership), cluster = factor(clust$membership))
betweenness <- betweenness(subnet) 
centrality <- degree(subnet) 
df$betweenness <- betweenness[as.character(df$gene_id)]
df$degree_centrality <- centrality[as.character(df$gene_id)]
df$betweenness <- as.integer(df$betweenness)
df$degree_centrality <- as.integer(df$degree_centrality)

rownames(df) <- 1:nrow(df)

df <- df[order(-df$degree_centrality), ]


df <- merge(gene_ensembl, df, by.x = "ensembl_gene_id", by.y = "gene_id")

# convert ensembl to uniprot
ensembl_to_uniprot <- getBM(attributes = c("ensembl_gene_id", "uniprot_gn_id"), 
                      filters = "ensembl_gene_id", 
                      values = df$ensembl_gene_id, 
                      mart = ensembl)

# convert external gene name to uniprot
gn_to_uniprot <- getBM(attributes = c("external_gene_name", "uniprot_gn_id"), 
                     filters = "external_gene_name", 
                     values = df$external_gene_name, 
                     mart = ensembl)

# get list of missing genes
missing_genes <- gn_to_uniprot[gn_to_uniprot$uniprot_gn_id == "", ]
missing_genes <- missing_genes$external_gene_name

PCSF_master <- merge(ensembl_to_uniprot, df, by = "ensembl_gene_id")
PCSF_master2 <- merge(gn_to_uniprot, df, by = "external_gene_name")


# load Fpocket data
af_drugability <- read.csv("../druggability_results/fpocket_druggability.csv")

# merge PCSF data with AF
PCSF_results <- merge(PCSF_master2, af_drugability, by.x = "uniprot_gn_id", by.y = "uniprot_id")

# run citation scoring tool and read in
citation_scores <- read.csv("citation_scores.csv")

PCSF_results <- merge(PCSF_results, citation_scores, by.x = "external_gene_name", by.y = "gene_id")
PCSF_results <- na.omit(PCSF_results) # for some reason the merge created empty duplicates.
PCSF_results <- PCSF_results[order(-PCSF_results$druggability), ]
rownames(PCSF_results) <- NULL

kable(PCSF_results, format = "html", caption = "PCSF results") %>%
kable_styling(full_width = FALSE) %>%
scroll_box(height = "400px")
```

<br>

<details>
<summary>Show missing genes</summary>
```{r missing genes, echo=FALSE}
missing_genes
```
</details>

<br>

<br>

### Ranking

The next section of this document is dedicated to the ranking of genes based on a combination of scoring methods. This ranking system can be easily interchangeable with other/new data types. Include here is the addition of cryptic pocket scores which were analysed in python using the same AlphaFold structures as done with Fpocket. Any structures less than 80 amino acids in length were removed per the PocketMiner methodology. Using a 10 amino acid "window" average, pockets are calculated per amino acid (also as per PocketMiner methodology).

The rank sensitivity test was not performed with the PocketMiner data becasue its not very easy to include new data types like the rank aggregation. Can be done if required but the first comparison shows they return similar results anyway so not much value in running again if rank aggregation method is far better. It also has the benefit of being accompanied by a comprehensive study for the method.

<br>

## {.tabset .tabset-pills}

### Robust Rank Aggregation

```{r Robust Rank Aggregation, echo=FALSE}
# Robust Rank Agrregation
betweenness <- subset(PCSF_results, select = c("ID", "betweenness"))
betweenness <- distinct(betweenness)
betweenness$betweenness <- (betweenness$betweenness - min(betweenness$betweenness)) / (max(betweenness$betweenness) - min(betweenness$betweenness))
betweenness <- betweenness[order(betweenness$betweenness, decreasing = T), ]
betweenness <- betweenness$ID

centrality <- subset(PCSF_results, select = c("ID", "degree_centrality"))
centrality <- distinct(centrality)
centrality$degree_centrality <- (centrality$degree_centrality - min(centrality$degree_centrality)) / (max(centrality$degree_centrality) - min(centrality$degree_centrality))
centrality <- centrality[order(centrality$degree_centrality, decreasing = T), ]
centrality <- centrality$ID

# log transformation of citation scores
citation <- subset(PCSF_results, select = c("ID", "citation_score"))
citation <- distinct(citation)
citation$citation_score <- log(citation$citation_score + 1)
citation$citation_score <- (citation$citation_score - min(citation$citation_score)) / (max(citation$citation_score) - min(citation$citation_score))
citation <- citation[order(citation$citation_score, decreasing = F), ]
citation <- citation$ID

druggability_data <- subset(PCSF_results, select = c("ID", "druggability", "num_drug_pockets"))
druggability_data <- distinct(druggability_data)
druggability <- druggability_data[order(druggability_data$druggability, decreasing = T), ]
druggability <- druggability$ID

num_drug_pockets <- druggability_data[order(druggability_data$num_drug_pockets, decreasing = T), ]
num_drug_pockets <- num_drug_pockets$ID

rankings <- list(betweenness, centrality, citation, druggability, num_drug_pockets)

aggregate_ranks <- aggregateRanks(glist = rankings)

# merge gene names back in
temp <- subset(PCSF_results, select = c("external_gene_name", "ID"))
aggregate_ranks <- merge(temp, aggregate_ranks, by.x = "ID", by.y = "Name")
rm(temp)

aggregate_ranks <- aggregate_ranks[order(aggregate_ranks$Score), ]
rownames(aggregate_ranks) <- NULL

# remove scientific notation
options(scipen=999)

kable(aggregate_ranks, format = "html", caption = "Robust Rank Aggregation") %>%
kable_styling(full_width = FALSE) %>%
scroll_box(height = "400px")
```

### Rank Sensitivity

```{r Rank Sensitivity, echo=FALSE}

betweeness_norm <- (PCSF_results$betweenness - min(PCSF_results$betweenness)) / (max(PCSF_results$betweenness) - min(PCSF_results$betweenness))

centrality_norm <- (PCSF_results$degree_centrality - min(PCSF_results$degree_centrality)) / (max(PCSF_results$degree_centrality) - min(PCSF_results$degree_centrality))

# log transformation
citation_norm <- log(PCSF_results$citation_score + 1)
citation_norm <- (citation_norm - min(citation_norm)) / (max(citation_norm) - min(citation_norm))


# Load the progress package
library(progress)

# Define the number of weight values to test
num_weights <- 11

# Create weight value sequences that sum up to 1
betweeness_w_values <- seq(0, 1, length.out = num_weights)
citation_w_values <- seq(0, 1, length.out = num_weights)
centrality_w_values <- seq(0, 1, length.out = num_weights)
druggability_w_values <- seq(0, 1, length.out = num_weights)

# Calculate the total number of iterations
total_iterations <- num_weights ^ 4

# Create a progress bar
pb <- progress_bar$new(total = total_iterations)

# Create an empty data frame with proper column names
sensitivity_results <- data.frame(
  betweeness_weight = numeric(),
  citation_weight = numeric(),
  centrality_weight = numeric(),
  druggability_weight = numeric(),
  top_genes = character(),
  stringsAsFactors = FALSE
)

# Perform sensitivity test
for (betweeness_w in betweeness_w_values) {
  for (citation_w in citation_w_values) {
    for (centrality_w in centrality_w_values) {
      for (druggability_w in druggability_w_values) {
        # Check if the weights sum up to 1
        if (betweeness_w + citation_w + centrality_w + druggability_w == 1) {
          # Combine scores using current weights
          combined_score <- betweeness_w * betweeness_norm +
            centrality_w * centrality_norm +
            druggability_w * PCSF_results$druggability -
            citation_w * citation_norm
          
          # Rank the genes based on the combined score
          PCSF_results_edit <- PCSF_results
          PCSF_results_edit$combined_score <- combined_score
          PCSF_results_edit <- PCSF_results_edit[order(-PCSF_results_edit$combined_score), ]
          
          # Select the top 10 genes
          top_genes <- head(PCSF_results_edit$external_gene_name, 10)
          
          # Add results to the sensitivity_results data frame
          sensitivity_results <- rbind(sensitivity_results, list(
            betweeness_weight = betweeness_w,
            citation_weight = citation_w,
            centrality_weight = centrality_w,
            druggability_weight = druggability_w,
            top_genes = paste(top_genes, collapse = ', ')
          ))
        }
        
        # Increment the progress bar
        pb$tick()
      }
    }
  }
}


# Split the "top_genes" column into a list of genes
sensitivity_results$top_genes_list <- strsplit(sensitivity_results$top_genes, ', ')

# Count the occurrences of each gene
all_genes <- unlist(sensitivity_results$top_genes_list)
all_genes_unique <- unique(all_genes)
count <- sapply(all_genes_unique, function(g) sum(sapply(sensitivity_results$top_genes_list, function(lst) g %in% lst)))

# Create an empty data frame to store gene counts
gene_counts <- data.frame(
  gene = all_genes_unique,
  count = count,
  stringsAsFactors = FALSE
)

# Sort the gene counts by count
gene_counts <- gene_counts[order(-gene_counts$count), ]



counts_when_betweeness_0 <- sensitivity_results[sensitivity_results$betweeness_weight == 0, ]
counts_when_betweeness_0 <- sapply(all_genes_unique, function(g) sum(sapply(counts_when_betweeness_0$top_genes_list, function(lst) g %in% lst)))

gene_counts <- cbind(gene_counts, counts_when_betweeness_0)

counts_when_centrality_0 <- sensitivity_results[sensitivity_results$centrality_weight == 0, ]
counts_when_centrality_0 <- sapply(all_genes_unique, function(g) sum(sapply(counts_when_centrality_0$top_genes_list, function(lst) g %in% lst)))

gene_counts <- cbind(gene_counts, counts_when_centrality_0)

counts_when_citation_0 <- sensitivity_results[sensitivity_results$citation_weight == 0, ]
counts_when_citation_0 <- sapply(all_genes_unique, function(g) sum(sapply(counts_when_citation_0$top_genes_list, function(lst) g %in% lst)))

gene_counts <- cbind(gene_counts, counts_when_citation_0)

counts_when_druggability_0 <- sensitivity_results[sensitivity_results$druggability_weight == 0, ]
counts_when_druggability_0 <- sapply(all_genes_unique, function(g) sum(sapply(counts_when_druggability_0$top_genes_list, function(lst) g %in% lst)))

gene_counts <- cbind(gene_counts, counts_when_druggability_0)

rownames(gene_counts) <- NULL

kable(gene_counts, format = "html", caption = "Gene Score Sensitivity Analysis") %>%
kable_styling(full_width = FALSE) %>%
scroll_box(height = "400px")
```

### Robust Rank Aggregation w Cryptic Pockets
```{r Robust Rank Aggregation w Cryptic Pockets, echo=FALSE}
# Robust Rank Aggregation
betweenness <- subset(PCSF_results, select = c("ID", "betweenness"))
betweenness <- distinct(betweenness)
betweenness$betweenness <- (betweenness$betweenness - min(betweenness$betweenness)) / (max(betweenness$betweenness) - min(betweenness$betweenness))
betweenness <- betweenness[order(betweenness$betweenness, decreasing = T), ]
betweenness <- betweenness$ID

centrality <- subset(PCSF_results, select = c("ID", "degree_centrality"))
centrality <- distinct(centrality)
centrality$degree_centrality <- (centrality$degree_centrality - min(centrality$degree_centrality)) / (max(centrality$degree_centrality) - min(centrality$degree_centrality))
centrality <- centrality[order(centrality$degree_centrality, decreasing = T), ]
centrality <- centrality$ID

# log transformation of citation scores
citation <- subset(PCSF_results, select = c("ID", "citation_score"))
citation <- distinct(citation)
citation$citation_score <- log(citation$citation_score + 1)
citation$citation_score <- (citation$citation_score - min(citation$citation_score)) / (max(citation$citation_score) - min(citation$citation_score))
citation <- citation[order(citation$citation_score, decreasing = F), ]
citation <- citation$ID

druggability_data <- subset(PCSF_results, select = c("ID", "druggability", "num_drug_pockets"))
druggability_data <- distinct(druggability_data)
druggability <- druggability_data[order(druggability_data$druggability, decreasing = T), ]
druggability <- druggability$ID

num_drug_pockets <- druggability_data[order(druggability_data$num_drug_pockets, decreasing = T), ]
num_drug_pockets <- num_drug_pockets$ID

# add the PocketMiner data as another ranking
pocketminer_data <- read.csv("../pocketminer/results/pocketminer_results.csv")
pocketminer_data <- pocketminer_data[pocketminer_data$ID %in% PCSF_results$ID, ]

pocketminer_data <- pocketminer_data[order(pocketminer_data$max_hit, decreasing = T), ]
cryptic_pockets <- pocketminer_data$ID

# reorder for num_hits
pocketminer_data <- pocketminer_data[order(pocketminer_data$num_hits, decreasing = T), ]
num_cryp_pockets <- pocketminer_data$ID

rankings <- list(betweenness, centrality, citation, druggability, num_drug_pockets, cryptic_pockets, num_cryp_pockets)


aggregate_ranks <- aggregateRanks(glist = rankings)

# merge gene names back in
temp <- subset(PCSF_results, select = c("external_gene_name", "ID"))
aggregate_ranks <- merge(temp, aggregate_ranks, by.x = "ID", by.y = "Name")
rm(temp)

aggregate_ranks <- aggregate_ranks[order(aggregate_ranks$Score), ]
rownames(aggregate_ranks) <- NULL

# remove scientific notation
options(scipen=999)

kable(aggregate_ranks, format = "html", caption = "Robust Rank Aggregation") %>%
kable_styling(full_width = FALSE) %>%
scroll_box(height = "400px")
```

##

<br>

If interested, can view correlation plots between druggability and cryptic pockets here:

<details>
<summary>Druggability VS Cryptic Pockets</summary>
```{r correlation plot, echo=FALSE, out.width='50%', fig.show='hold'}
# this code chunk is required to be run on Ubuntu. removes small structures (<80aa) from PCSF_results
load("RData/correlation_plot_data.RData")

IDs <- c()

for (id in small_structs){
  id <- strsplit(id, "-")
  id <- unlist(id)
  id <- paste0(id[1], "-", id[2])
  IDs <- append(IDs, id)
}

PCSF_results_updated <- PCSF_results[!(PCSF_results$ID %in% IDs), ]

plot_data <- pocketminer_data[pocketminer_data$ID %in% PCSF_results_updated$ID, ]

plot_data <- data.frame(num_cryp = plot_data$num_hits,
                        cryp = plot_data$max_hit,
                        num_drug = PCSF_results_updated$num_drug_pockets,
                        drug = PCSF_results_updated$druggability)

ggplot(plot_data, aes(x = num_drug, y = num_cryp)) +
  geom_point()

ggplot(plot_data, aes(x = drug, y = cryp)) +
  geom_point()
```
</details>

<br>

<br>

end
